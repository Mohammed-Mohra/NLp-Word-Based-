{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvNcMOOGyVZwIJHlxXCg5v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mohammed-Mohra/NLp-Word-Based-/blob/main/NL_word_Based.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Big53VgStxr4"
      },
      "outputs": [],
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import glob\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import unicodedata\n",
        "import string\n",
        "import numpy\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import keras\n",
        "import string\n",
        "import nltk\n",
        "import gensim, logging\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "#from keras_preprocessing.sequence import pad_sequences\n",
        "\n",
        "string.punctuation = string.punctuation +'“'+'”'+'-'+'’'+'‘'+'—'\n",
        "string.punctuation = string.punctuation.replace('.', '') #why replace the dot with empty shit. Because we want to get senteneces make senseb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n"
      ],
      "metadata": {
        "id": "8tShYILluKSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSc-ry-6vlBo",
        "outputId": "12eafe10-a860-46ba-adbe-517c2d79bf0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dIgzdeMXwQoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loads the data and preprocesses data and stores corpus in raw_text\n",
        "raw_text = open('/content/gdrive/MyDrive/corpus (3).txt', encoding = 'utf8').read()\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# load stopwords\n",
        "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "file_nl_remo = \"\"\n",
        "for line in raw_text:\n",
        "  new_line = line.replace(\"\\n\", \" \")           \n",
        "#removes newlines\n",
        "  file_nl_remo += new_line\n",
        "\n",
        "#new_file = new_file.lower()   \n",
        " \n",
        "file_p = re.sub(\"[^A-Za-z'.]+\", ' ',file_nl_remo).lower()   \n",
        "\n",
        "#removes all special characters\n",
        "sents = nltk.sent_tokenize(file_p)\n",
        "print(\"The number of sentences is\", len(sents)) \n",
        "#prints the number of sentences\n",
        "\n",
        "string.punctuation = string.punctuation + '.' # why did we add the dot now \n",
        "file_p = re.sub(\"[^A-Za-z']+\", ' ',file_p).lower()   \n",
        "\n",
        "\n",
        "words = nltk.word_tokenize(file_p)\n",
        "words = [word for word in words if word not in stop_words]\n",
        "\n",
        "print(\"The number of tokens is\", len(words)) \n",
        "#prints the number of tokens\n",
        "\n",
        "average_tokens = round(len(words)/len(sents))\n",
        "print(\"The average number of tokens per sentence is\", average_tokens) \n",
        "#prints the average number of tokens per sentence\n",
        "\n",
        "unique_tokens = set(words)\n",
        "print(\"The number of unique tokens are\", len(unique_tokens)) \n",
        "#prints the number of unique tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kn2Pl1VEuMEQ",
        "outputId": "e593dd1e-51db-4fdc-e88d-3a387974af7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of sentences is 982\n",
            "The number of tokens is 12239\n",
            "The average number of tokens per sentence is 12\n",
            "The number of unique tokens are 2421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters of the model\n",
        "\n",
        "vocab_size = 2450  #chosen based on statistics of the model\n",
        "\n",
        "oov_tok = '<OOV>'\n",
        "\n",
        "embedding_dim = 100\n",
        "\n",
        "padding_type='post'\n",
        "\n",
        "trunc_type='post'\n",
        "# tokenizes sentences\n",
        "\n",
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
        "\n",
        "tokenizer.fit_on_texts([file_p])\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "seq_length = 50\n",
        "\n",
        "tokens = tokenizer.texts_to_sequences([file_p])[0]"
      ],
      "metadata": {
        "id": "oeILWiSfuO0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataX = []\n",
        "dataY = []\n",
        "\n",
        "seq_length = 50\n",
        "\n",
        "for i in range(0, len(tokens) - seq_length-1 ):\n",
        "    seq_in = tokens[i:i + seq_length] # The 50 word sequence\n",
        "    seq_out = tokens[i + seq_length] # The word that we want to predict \n",
        "\n",
        "    if seq_out==1: #Skip samples where target word is OOV\n",
        "        continue\n",
        "    \n",
        "    dataX.append(seq_in)\n",
        "    dataY.append(seq_out)\n",
        "\n",
        "X = numpy.array(dataX)\n",
        "y = numpy.array(dataY)\n",
        "\n",
        "X_tensor = torch.tensor(X)\n",
        "Y_tensor = torch.Tensor(y)\n",
        "#print (\"Total training data size is -\", N)\n",
        "#X = word_vectors\n",
        "#X = numpy.array(dataX)\n",
        "\n",
        "# one hot encodes the output variable\n",
        "#y = numpy.array(dataY)\n",
        "#y = np_utils.to_categorical(dataY)"
      ],
      "metadata": {
        "id": "enfz9YDcwhsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 100 # what is the hidden dimension \n",
        "LAY_NUM = 2\n",
        "BATCH_SIZE = 1\n",
        "class LSTM_model(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size ):\n",
        "        super(LSTM_model, self).__init__()\n",
        "        self.hidden_dim = HIDDEN_DIM\n",
        "        self.seq_len = 50\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "        # with dimensionality hidden_dim.\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim ,num_layers = LAY_NUM, bidirectional = True)\n",
        "\n",
        "        # The linear layer that maps from hidden state space to tag space\n",
        "        self.final_output = nn.Linear(2*HIDDEN_DIM, vocab_size)\n",
        "        self.softmax = nn.LogSoftmax()\n",
        "        \n",
        "    def forward(self, word, h_c):\n",
        "        h_o = h_c[0]  \n",
        "        c_o = h_c[1]\n",
        "        embeds = self.word_embeddings(word)\n",
        "        lstm_out, (h_next,c_next) = self.lstm(embeds.view( self.seq_len, BATCH_SIZE,embeds.shape[-1]), (h_o,c_o))\n",
        "        lstm_out = lstm_out[49,:]\n",
        "        lstm_out = self.final_output(lstm_out )\n",
        "\n",
        "       # lstm_out = torch.relu(lstm_out)\n",
        "        #print(lstm_out[:20])\n",
        "        lstm_out = self.softmax(lstm_out)\n",
        "        #print(lstm_out.shape)\n",
        "\n",
        "        return lstm_out, (h_next,c_next) # check if lstm_out is same as h_next they should be identical \n",
        "    \n",
        "    def initHidden(self):\n",
        "        return torch.zeros(2*LAY_NUM,BATCH_SIZE, self.hidden_dim)"
      ],
      "metadata": {
        "id": "CtQ6W8M-wirG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.NLLLoss()\n"
      ],
      "metadata": {
        "id": "v8LYzhz_wl_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001 # If you set this too high, it might explode. If too low, it might not learn\n",
        "lstm_test = 0\n",
        "y_test = 0\n",
        "\n",
        "def train(category_tensor, line_tensor):\n",
        "    h_next = LSTM_model1.initHidden()\n",
        "    c_next = h_next\n",
        "    \n",
        "    LSTM_model1.zero_grad()\n",
        "\n",
        "    #for i in range(seq_length):\n",
        "    lstm_out, (h_next,c_next) = LSTM_model1(line_tensor,( h_next, c_next)) # Only the final output is the important one we can ignore all others     \n",
        "    lstm_test = lstm_out\n",
        "   # out_index = lstm_out.argmax().unsqueeze(0).long()\n",
        "    category_tensor = category_tensor.long()\n",
        "    y_test = category_tensor\n",
        "   \n",
        "#.view(BATCH_SIZE,len(lstm_out))\n",
        "    loss = criterion(lstm_out, category_tensor)\n",
        "    loss.backward()\n",
        "    # Add parameters' gradients to their values, multiplied by learning rate\n",
        "    # This is a mystery what is happening cause updating the parameters seems to be not automatic\n",
        "    optimizer = optim.Adam(LSTM_model1.parameters(), lr= learning_rate)\n",
        "    optimizer.step()\n",
        "    \n",
        "   \n",
        "    return lstm_out, loss.item()"
      ],
      "metadata": {
        "id": "IXunqx9rwpd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_tensor.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "php4cK5i2pSh",
        "outputId": "83d0fe00-5d80-4907-fca2-5d146bc8325d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([27164])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_tensor.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpSQH9w_9bJA",
        "outputId": "989c2be5-7373-41cc-ffea-3e4aa098e8ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([27164, 50])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LSTM_model1 = LSTM_model( EMBEDDING_DIM, HIDDEN_DIM, vocab_size)\n"
      ],
      "metadata": {
        "id": "_b-Ey78Rx8zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCH_NUM = 50\n",
        "for j in range(EPOCH_NUM):\n",
        "  for i in range(212):\n",
        "      _ , loss = train(Y_tensor[i*BATCH_SIZE:i*BATCH_SIZE + BATCH_SIZE], X_tensor[i*BATCH_SIZE:i*BATCH_SIZE + BATCH_SIZE])\n",
        "      if(i %10 ==0):\n",
        "        print(loss)\n",
        "     \n",
        "  print('------------------------------------------------------------------------------------------------------------')\n",
        "  print(f'This is Epoch number {j}')\n",
        "  print('------------------------------------------------------------------------------------------------------------')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMJxJNx6wtKU",
        "outputId": "eeae8f4e-a13b-404b-8260-4ae7812c1948"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-ee168907982b>:31: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  lstm_out = self.softmax(lstm_out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.397036075592041\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "reverse_word_map"
      ],
      "metadata": {
        "id": "qXJkxzObpvry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creates word to idx map using tokenizer.word_index\n",
        "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "# Returns the next n words greedily\n",
        "def next_tokens(input_str, n):\n",
        "    Y_index = 0\n",
        "    h_next = LSTM_model1.initHidden()\n",
        "    c_next = h_next\n",
        "    \n",
        "    print (\"Seed -\",  input_str, sep = '\\n\\n')\n",
        "    final_string = ''\n",
        "    real_string = ''\n",
        "    for i in range(n):\n",
        "        #string.punctuation = string.punctuation.replace('.', '')\n",
        "        #file_q = \"\".join([char for char in input_str if char not in string.punctuation])   #removes even periods.\n",
        "        token = tokenizer.texts_to_sequences([input_str])[0]\n",
        "        X_tensor = torch.tensor(token)\n",
        "        X_tensor = X_tensor[:50]\n",
        "       # print(X_tensor.shape)\n",
        "        #token = numpy.array(token ).reshape((1,-1))\n",
        "       # print(token.shape)\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "          prediction,_ = LSTM_model1(X_tensor, ( h_next, c_next))\n",
        "          prediction = prediction.argmax(dim = 1)\n",
        "         # print(prediction)\n",
        "         # print(prediction.shape)\n",
        "          final_string = final_string + reverse_word_map[int(prediction)] + ' ' \n",
        "          input_str = input_str + ' ' + reverse_word_map[int(prediction)]\n",
        "          input_str = ' '.join(input_str.split(' ')[1:])\n",
        "\n",
        "    \n",
        "    return final_string, real_string"
      ],
      "metadata": {
        "id": "iAoCzZVypsiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([4])\n",
        "int(a)\n",
        "#reverse_word_map[a[0]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLp_TBV2xWvT",
        "outputId": "0af641d4-57e7-43fe-e209-cfa3ee26c967"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_str = \"The boy laughed at the fright he had caused. This time, the villagers left angrily. The third day, as the boy went up\\\n",
        " the small hill, he suddenly saw a wolf attacking his sheep. He cried as hard as he could, “Wolf! Wolf! Wolf!”, but not \\\n",
        " a single villager came to help him. The villagers thought that he was trying to fool them again and did not come to rescue \\\n",
        " him or his sheep.\"  \n",
        "output = next_tokens(input_str, 40)\n",
        "print(\"\\nGenerated string -\\n\\n\", output)\n",
        "#print(\"\\Real string -\\n\\n\", real_str)\n",
        "#speficies an unseen input string\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArvL5AAzneYE",
        "outputId": "78c04b55-8122-4d34-91de-3a92e533388d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed -\n",
            "\n",
            "The boy laughed at the fright he had caused. This time, the villagers left angrily. The third day, as the boy went up the small hill, he suddenly saw a wolf attacking his sheep. He cried as hard as he could, “Wolf! Wolf! Wolf!”, but not  a single villager came to help him. The villagers thought that he was trying to fool them again and did not come to rescue  him or his sheep.\n",
            "\n",
            "Generated string -\n",
            "\n",
            " ('to be be be up on her the of and and to to this she alice and with alice out was was the the said there was she she she she very it it it and and the the in ', '')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-ee168907982b>:31: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  lstm_out = self.softmax(lstm_out)\n"
          ]
        }
      ]
    }
  ]
}